{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from recommender import recommender\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import creds  # Import\n",
    "import requests\n",
    "from euclidean_recommender import recommender_euclidean\n",
    "from kmeans_recommender import main\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import each playlist representing a specific mood\n",
    "drake_sad = pd.read_csv('playlist_1.csv')\n",
    "drake_hype = pd.read_csv('playlist_2.csv')\n",
    "drake_chill = pd.read_csv('playlist_3.csv')\n",
    "drake_romantic = pd.read_csv('playlist_4.csv')\n",
    "drake_party = pd.read_csv('playlist_5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to Evalutate Recommendation system:\n",
    "1. Split data into train and test\n",
    "2. Run the recommendations on each song in the playlist, add recommendations to a recommendation list\n",
    "3. Check if the recommendations are accurate with the test data\n",
    "4. Compute metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split the dataset into training and testing\n",
    "def split_dataset(df, train_ratio=0.7):\n",
    "    num_rows = len(df)\n",
    "    num_train = int(num_rows * train_ratio)\n",
    "    \n",
    "    # Shuffle the DataFrame rows\n",
    "    shuffled_df = df.sample(frac=1, random_state=42)\n",
    "    \n",
    "    # Split into training and testing DataFrames\n",
    "    train_df = shuffled_df.iloc[:num_train]\n",
    "    test_df = shuffled_df.iloc[num_train:]\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "sad_train, sad_test = split_dataset(drake_sad)\n",
    "hype_train, hype_test = split_dataset(drake_hype)\n",
    "chill_train, chill_test = split_dataset(drake_chill)\n",
    "romantic_train, romantic_test = split_dataset(drake_romantic)\n",
    "party_train, party_test = split_dataset(drake_party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in dataset containing all drake songs\n",
    "drake_df = pd.read_csv('drake_songs_dataset.csv')\n",
    "\n",
    "#Get desired audio features\n",
    "selected_features = [\n",
    "    'danceability', 'energy', 'key', 'loudness',\n",
    "    'speechiness', 'acousticness', 'instrumentalness', \n",
    "    'liveness', 'valence', 'tempo'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that scales all data before computing cosine similarity matrix\n",
    "def scale_data(input_song, drake_df):\n",
    "    # Making a copy to not alter drake_df\n",
    "    recommender_dataset = drake_df.copy()\n",
    "    \n",
    "    # Removing input song from recommender_dataset so it isn't recommended\n",
    "    recommender_dataset = recommender_dataset[recommender_dataset['track_uri'] != input_song['track_uri']]\n",
    "\n",
    "    #Getting only necessary columns before concat\n",
    "    recommender_dataset = recommender_dataset[selected_features].copy()\n",
    "    input_song = input_song[selected_features].copy().to_frame().T\n",
    "    \n",
    "    #Combining rows for features scaling\n",
    "    all_features = pd.concat([input_song, recommender_dataset])\n",
    "    scaler = StandardScaler()\n",
    "    all_features_scaled = scaler.fit_transform(all_features)\n",
    "\n",
    "    user_features = all_features_scaled[:1, :].copy()\n",
    "    dataset_features = all_features_scaled[1:, :].copy()\n",
    "    \n",
    "    return user_features, dataset_features\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that runs the recommendation system using cosine similarity\n",
    "def make_recs_cosine(input_song_index, playlist_df, drake_df):\n",
    "    #Keeps all columns so that we can extract the recommended song names and artists later\n",
    "    df_all_cols = drake_df.copy()\n",
    "\n",
    "    # Get the input song that we will make recommendations from\n",
    "    input_song = playlist_df.iloc[input_song_index]\n",
    "\n",
    "    #Remove the user's inputted track from original dataset so it isn't recommended later on\n",
    "    drake_df =  drake_df[drake_df['track_uri'] != input_song['track_uri']]\n",
    "\n",
    "    #Scale data\n",
    "    user_features, dataset_features = scale_data(input_song, drake_df)\n",
    "\n",
    "    # Recommending system\n",
    "    return recommender(user_features, dataset_features, df_all_cols, 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that runs the recommendation system using Euclidean Distance\n",
    "def make_recs_euclidean(input_song_index, playlist_df, drake_df):\n",
    "    #Keeps all columns so that we can extract the recommended song names and artists later\n",
    "    df_all_cols = drake_df.copy()\n",
    "\n",
    "    # Get the input song that we will make recommendations from\n",
    "    input_song = playlist_df.iloc[input_song_index]\n",
    "\n",
    "    #Remove the user's inputted track from original dataset so it isn't recommended later on\n",
    "    drake_df =  drake_df[drake_df['track_uri'] != input_song['track_uri']]\n",
    "\n",
    "    #Scale data\n",
    "    user_features, dataset_features = scale_data(input_song, drake_df)\n",
    "\n",
    "    # Recommending system\n",
    "    return recommender_euclidean(user_features, dataset_features, df_all_cols, 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that runs recommendation system using KMeans\n",
    "def make_recs_kmeans(input_song_index, df):\n",
    "\n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df[selected_features])\n",
    "\n",
    "    # Perform KMeans\n",
    "    num_clusters = 5\n",
    "    kmeans = KMeans(n_clusters = num_clusters, random_state = 42, n_init = 10)\n",
    "    kmeans.fit(scaled_features)\n",
    "\n",
    "    # Identify the cluster to which the target song belongs\n",
    "    target_song_features = scaled_features[input_song_index]  # Replace target_song_index with the index of your target song\n",
    "    target_song_cluster = kmeans.predict([target_song_features])[0]\n",
    "   \n",
    "    # Find songs in the same cluster as the target song\n",
    "    df['cluster'] = kmeans.labels_\n",
    "    \n",
    "    songs_in_same_cluster = df[df['cluster'] == target_song_cluster]\n",
    "\n",
    "    # Now you can recommend songs from the same cluster\n",
    "    recommended_songs = songs_in_same_cluster.sample(n=5, replace = True)\n",
    "    return recommended_songs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(recommended_songs, test_songs):\n",
    "    # Convert the recommended and test songs to sets for efficient comparison\n",
    "    recommended_set = set(recommended_songs)\n",
    "    test_set = set(test_songs)\n",
    "    \n",
    "    # Count the number of correct recommendations (intersection of sets)\n",
    "    num_correct_recommendations = len(recommended_set & test_set)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = num_correct_recommendations / len(test_set)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def calculate_precision(recommended_songs, test_songs):\n",
    "    recommended_set = set(recommended_songs)\n",
    "    test_set = set(test_songs)\n",
    "    \n",
    "    true_positives = len(recommended_set & test_set)\n",
    "    false_positives = len(recommended_set - test_set)\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    return precision\n",
    "\n",
    "def calculate_recall(recommended_songs, test_songs):\n",
    "    recommended_set = set(recommended_songs)\n",
    "    test_set = set(test_songs)\n",
    "    \n",
    "    true_positives = len(recommended_set & test_set)\n",
    "    false_negatives = len(test_set - recommended_set)\n",
    "    \n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    return recall\n",
    "\n",
    "def calculate_f1(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 2-4: Running the recommendation system and testing performance of Cosine Similiarity recommending system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playlist 1\n",
      "-------------------\n",
      "Accuracy: 0.55\n",
      "Precision: 0.05\n",
      "Recall: 0.55\n",
      "F1 score: 0.1\n",
      "-------------------\n",
      "Playlist 2\n",
      "-------------------\n",
      "Accuracy: 0.56\n",
      "Precision: 0.08\n",
      "Recall: 0.56\n",
      "F1 score: 0.14\n",
      "-------------------\n",
      "Playlist 3\n",
      "-------------------\n",
      "Accuracy: 0.69\n",
      "Precision: 0.16\n",
      "Recall: 0.69\n",
      "F1 score: 0.27\n",
      "-------------------\n",
      "Playlist 4\n",
      "-------------------\n",
      "Accuracy: 0.75\n",
      "Precision: 0.22\n",
      "Recall: 0.75\n",
      "F1 score: 0.34\n",
      "-------------------\n",
      "Playlist 5\n",
      "-------------------\n",
      "Accuracy: 0.74\n",
      "Precision: 0.26\n",
      "Recall: 0.74\n",
      "F1 score: 0.38\n",
      "-------------------\n",
      "Mean Accuracy: 0.658\n",
      "Mean Precision: 0.154\n",
      "Mean Recall: 0.658\n",
      "Mean F1 score: 0.076\n"
     ]
    }
   ],
   "source": [
    "NUM_PLAYLISTS = 5\n",
    "\n",
    "# List of playlists (dataframes) to test recommendation performance on\n",
    "playlist_list = [drake_sad, drake_hype, drake_chill, drake_romantic, drake_party]\n",
    "\n",
    "#Empty lists to calculate average metrics later\n",
    "accuracy_sum = 0\n",
    "precision_sum = 0\n",
    "recall_sum = 0\n",
    "f1_sum = 0\n",
    "\n",
    "# Run the recommendation system on every song in the list for each playlist\n",
    "for j, playlist in enumerate(playlist_list):\n",
    "    #Splitting into training and testing data\n",
    "    training_data, testing_data = split_dataset(playlist)\n",
    "\n",
    "    # Create empty list to store recommendations\n",
    "    recommendations = []\n",
    "\n",
    "    # Running recommendation system\n",
    "    for i, row in playlist.iterrows():\n",
    "        recommendations += make_recs_cosine(i, playlist, drake_df)['track_name'].to_list()\n",
    "\n",
    "    # Outputting accuracy for playlist\n",
    "    print(f\"Playlist {j + 1}\")\n",
    "    print(\"-------------------\")    \n",
    "    print(f\"Accuracy: {round(calculate_accuracy(recommendations, testing_data['track_name']),2 )}\\nPrecision: {round(calculate_precision(recommendations, testing_data['track_name']), 2)}\\nRecall: {round(calculate_recall(recommendations, testing_data['track_name']), 2)}\\nF1 score: {round(calculate_f1(calculate_precision(recommendations, testing_data['track_name']), calculate_recall(recommendations, testing_data['track_name'])), 2)}\")\n",
    "    print(\"-------------------\")    \n",
    "\n",
    "    #Storing metrics for later\n",
    "    accuracy_sum += round(calculate_accuracy(recommendations, testing_data['track_name']), 2)\n",
    "    precision_sum += round(calculate_precision(recommendations, testing_data['track_name']), 2)\n",
    "    recall_sum += round(calculate_recall(recommendations, testing_data['track_name']), 2)\n",
    "    f1_sum = round(calculate_f1(calculate_precision(recommendations, testing_data['track_name']), calculate_recall(recommendations, testing_data['track_name'])), 2)\n",
    "\n",
    "# Aggregate metrics\n",
    "print(f\"Mean Accuracy: {accuracy_sum / NUM_PLAYLISTS}\")\n",
    "print(f\"Mean Precision: {precision_sum / NUM_PLAYLISTS}\")\n",
    "print(f\"Mean Recall: {recall_sum / NUM_PLAYLISTS}\")\n",
    "print(f\"Mean F1 score: {f1_sum / NUM_PLAYLISTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 2-4: Running the recommendation system and testing performance of Euclidean Distiance recommending system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playlist 1\n",
      "-------------------\n",
      "Accuracy: 0.64\n",
      "Precision: 0.06\n",
      "Recall: 0.64\n",
      "F1 score: 0.11\n",
      "-------------------\n",
      "Playlist 2\n",
      "-------------------\n",
      "Accuracy: 0.58\n",
      "Precision: 0.09\n",
      "Recall: 0.58\n",
      "F1 score: 0.15\n",
      "-------------------\n",
      "Playlist 3\n",
      "-------------------\n",
      "Accuracy: 0.7\n",
      "Precision: 0.17\n",
      "Recall: 0.7\n",
      "F1 score: 0.27\n",
      "-------------------\n",
      "Playlist 4\n",
      "-------------------\n",
      "Accuracy: 0.77\n",
      "Precision: 0.23\n",
      "Recall: 0.77\n",
      "F1 score: 0.35\n",
      "-------------------\n",
      "Playlist 5\n",
      "-------------------\n",
      "Accuracy: 0.73\n",
      "Precision: 0.26\n",
      "Recall: 0.73\n",
      "F1 score: 0.38\n",
      "-------------------\n",
      "Mean Accuracy: 0.6839999999999999\n",
      "Mean Precision: 0.162\n",
      "Mean Recall: 0.6839999999999999\n",
      "Mean F1 score: 0.076\n"
     ]
    }
   ],
   "source": [
    "NUM_PLAYLISTS = 5\n",
    "\n",
    "# List of playlists (dataframes) to test recommendation performance on\n",
    "playlist_list = [drake_sad, drake_hype, drake_chill, drake_romantic, drake_party]\n",
    "\n",
    "#Empty lists to calculate average metrics later\n",
    "accuracy_sum = 0\n",
    "precision_sum = 0\n",
    "recall_sum = 0\n",
    "f1_sum = 0\n",
    "\n",
    "# Run the recommendation system on every song in the list for each playlist\n",
    "for j, playlist in enumerate(playlist_list):\n",
    "    #Splitting into training and testing data\n",
    "    training_data, testing_data = split_dataset(playlist)\n",
    "\n",
    "    # Create empty list to store recommendations\n",
    "    recommendations = []\n",
    "\n",
    "    # Running recommendation system\n",
    "    for i, row in playlist.iterrows():\n",
    "        recommendations += make_recs_euclidean(i, playlist, drake_df)['track_name'].to_list()\n",
    "\n",
    "    # Outputting accuracy for playlist\n",
    "    print(f\"Playlist {j + 1}\")\n",
    "    print(\"-------------------\")    \n",
    "    print(f\"Accuracy: {round(calculate_accuracy(recommendations, testing_data['track_name']),2 )}\\nPrecision: {round(calculate_precision(recommendations, testing_data['track_name']), 2)}\\nRecall: {round(calculate_recall(recommendations, testing_data['track_name']), 2)}\\nF1 score: {round(calculate_f1(calculate_precision(recommendations, testing_data['track_name']), calculate_recall(recommendations, testing_data['track_name'])), 2)}\")\n",
    "    print(\"-------------------\")    \n",
    "\n",
    "    #Storing metrics for later\n",
    "    accuracy_sum += round(calculate_accuracy(recommendations, testing_data['track_name']), 2)\n",
    "    precision_sum += round(calculate_precision(recommendations, testing_data['track_name']), 2)\n",
    "    recall_sum += round(calculate_recall(recommendations, testing_data['track_name']), 2)\n",
    "    f1_sum = round(calculate_f1(calculate_precision(recommendations, testing_data['track_name']), calculate_recall(recommendations, testing_data['track_name'])), 2)\n",
    "\n",
    "# Aggregate metrics\n",
    "print(f\"Mean Accuracy: {accuracy_sum / NUM_PLAYLISTS}\")\n",
    "print(f\"Mean Precision: {precision_sum / NUM_PLAYLISTS}\")\n",
    "print(f\"Mean Recall: {recall_sum / NUM_PLAYLISTS}\")\n",
    "print(f\"Mean F1 score: {f1_sum / NUM_PLAYLISTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 2-4: Running the recommendation system and testing performance of KMeans recommending system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playlist 1\n",
      "-------------------\n",
      "Accuracy: 1.0\n",
      "Precision: 0.31\n",
      "Recall: 1.0\n",
      "F1 score: 0.47\n",
      "-------------------\n",
      "Playlist 2\n",
      "-------------------\n",
      "Accuracy: 1.0\n",
      "Precision: 0.3\n",
      "Recall: 1.0\n",
      "F1 score: 0.46\n",
      "-------------------\n",
      "Playlist 3\n",
      "-------------------\n",
      "Accuracy: 1.0\n",
      "Precision: 0.35\n",
      "Recall: 1.0\n",
      "F1 score: 0.52\n",
      "-------------------\n",
      "Playlist 4\n",
      "-------------------\n",
      "Accuracy: 1.0\n",
      "Precision: 0.39\n",
      "Recall: 1.0\n",
      "F1 score: 0.56\n",
      "-------------------\n",
      "Playlist 5\n",
      "-------------------\n",
      "Accuracy: 1.0\n",
      "Precision: 0.41\n",
      "Recall: 1.0\n",
      "F1 score: 0.58\n",
      "-------------------\n",
      "Mean Accuracy: 1.0\n",
      "Mean Precision: 0.352\n",
      "Mean Recall: 1.0\n",
      "Mean F1 score: 0.11599999999999999\n"
     ]
    }
   ],
   "source": [
    "NUM_PLAYLISTS = 5\n",
    "\n",
    "# List of playlists (dataframes) to test recommendation performance on\n",
    "playlist_list = [drake_sad, drake_hype, drake_chill, drake_romantic, drake_party]\n",
    "\n",
    "#Empty lists to calculate average metrics later\n",
    "accuracy_sum = 0\n",
    "precision_sum = 0\n",
    "recall_sum = 0\n",
    "f1_sum = 0\n",
    "\n",
    "# Run the recommendation system on every song in the list for each playlist\n",
    "for j, playlist in enumerate(playlist_list):\n",
    "    #Splitting into training and testing data\n",
    "    training_data, testing_data = split_dataset(playlist)\n",
    "\n",
    "    # Create empty list to store recommendations\n",
    "    recommendations = []\n",
    "    \n",
    "    # Running recommendation system\n",
    "    for i, row in playlist.iterrows():\n",
    "        recommendations += make_recs_kmeans(i, playlist)['track_name'].to_list()\n",
    "\n",
    "    # Outputting accuracy for playlist\n",
    "    print(f\"Playlist {j + 1}\")\n",
    "    print(\"-------------------\")    \n",
    "    print(f\"Accuracy: {round(calculate_accuracy(recommendations, testing_data['track_name']),2 )}\\nPrecision: {round(calculate_precision(recommendations, testing_data['track_name']), 2)}\\nRecall: {round(calculate_recall(recommendations, testing_data['track_name']), 2)}\\nF1 score: {round(calculate_f1(calculate_precision(recommendations, testing_data['track_name']), calculate_recall(recommendations, testing_data['track_name'])), 2)}\")\n",
    "    print(\"-------------------\")    \n",
    "\n",
    "    #Storing metrics for later\n",
    "    accuracy_sum += round(calculate_accuracy(recommendations, testing_data['track_name']), 2)\n",
    "    precision_sum += round(calculate_precision(recommendations, testing_data['track_name']), 2)\n",
    "    recall_sum += round(calculate_recall(recommendations, testing_data['track_name']), 2)\n",
    "    f1_sum = round(calculate_f1(calculate_precision(recommendations, testing_data['track_name']), calculate_recall(recommendations, testing_data['track_name'])), 2)\n",
    "\n",
    "# Aggregate metrics\n",
    "print(f\"Mean Accuracy: {accuracy_sum / NUM_PLAYLISTS}\")\n",
    "print(f\"Mean Precision: {precision_sum / NUM_PLAYLISTS}\")\n",
    "print(f\"Mean Recall: {recall_sum / NUM_PLAYLISTS}\")\n",
    "print(f\"Mean F1 score: {f1_sum / NUM_PLAYLISTS}\")b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=5, n_init=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=5, n_init=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=5, n_init=10, random_state=42)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling data\n",
    "data = drake_df[selected_features]\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "#Fitting model\n",
    "model = KMeans(n_clusters = 5, random_state = 42, n_init = 10)\n",
    "model.fit(scaled_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but KMeans was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Divide \"sad song\" playlist into input and true recommended song groups\n",
    "input_group, true_recommended_group = train_test_split(drake_sad[selected_features], test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 2: Train the KMeans model\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(drake_df[selected_features])\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(scaled_features)\n",
    "\n",
    " # Find songs in the same cluster as the target song\n",
    "drake_df['cluster'] = kmeans.labels_\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'track_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'track_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m recommendations \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m true_recommended_songs \u001b[39m=\u001b[39m true_recommended_group[\u001b[39m'\u001b[39;49m\u001b[39mtrack_name\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m input_song_index, input_song \u001b[39min\u001b[39;00m input_group\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m      4\u001b[0m     target_song_features \u001b[39m=\u001b[39m scaled_features[input_song_index]\n",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'track_name'"
     ]
    }
   ],
   "source": [
    "recommendations = []\n",
    "true_recommended_songs = true_recommended_group['track_name']\n",
    "for input_song_index, input_song in input_group.iterrows():\n",
    "    target_song_features = scaled_features[input_song_index]\n",
    "    target_song_cluster = kmeans.predict([target_song_features])[0]\n",
    "\n",
    "    songs_in_same_cluster = drake_df[drake_df['cluster'] == target_song_cluster]\n",
    "    recommendations += songs_in_same_cluster.sample(n=5)['track_name'].tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [0, 18]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m f1_score(recommendations, true_recommended_songs)\n",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1238\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1070\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[0;32m   1071\u001b[0m     {\n\u001b[0;32m   1072\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msparse matrix\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1096\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1097\u001b[0m ):\n\u001b[0;32m   1098\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \n\u001b[0;32m   1100\u001b[0m \u001b[39m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[39m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1237\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1238\u001b[0m     \u001b[39mreturn\u001b[39;00m fbeta_score(\n\u001b[0;32m   1239\u001b[0m         y_true,\n\u001b[0;32m   1240\u001b[0m         y_pred,\n\u001b[0;32m   1241\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   1242\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1243\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1244\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1245\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1246\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1247\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    186\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1411\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[0;32m   1251\u001b[0m     {\n\u001b[0;32m   1252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msparse matrix\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1279\u001b[0m ):\n\u001b[0;32m   1280\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m \n\u001b[0;32m   1282\u001b[0m \u001b[39m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[39m    0.38...\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1411\u001b[0m     _, _, f, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1412\u001b[0m         y_true,\n\u001b[0;32m   1413\u001b[0m         y_pred,\n\u001b[0;32m   1414\u001b[0m         beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[0;32m   1415\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1416\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1417\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1418\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mf-score\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[0;32m   1419\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1420\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1421\u001b[0m     )\n\u001b[0;32m   1422\u001b[0m     \u001b[39mreturn\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    186\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1721\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1563\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1564\u001b[0m \n\u001b[0;32m   1565\u001b[0m \u001b[39mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1718\u001b[0m \u001b[39m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1719\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m zero_division_value \u001b[39m=\u001b[39m _check_zero_division(zero_division)\n\u001b[1;32m-> 1721\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[0;32m   1723\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1499\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m average_options \u001b[39mand\u001b[39;00m average \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39maverage has to be one of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1499\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   1500\u001b[0m \u001b[39m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[39m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m present_labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     85\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kevoi\\OneDrive\\Desktop\\Coding\\ds-projects\\spotify-recs\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 18]"
     ]
    }
   ],
   "source": [
    "f1_score(recommendations, true_recommended_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Search & Rescue',\n",
       " 'Club Paradise',\n",
       " 'Lose You',\n",
       " 'Fire & Desire',\n",
       " 'From Florida With Love',\n",
       " 'Trust Issues',\n",
       " 'The Real Her',\n",
       " 'My Side',\n",
       " 'Take Care',\n",
       " 'Can I',\n",
       " 'Too Much',\n",
       " 'Teenage Fever',\n",
       " 'Marvins Room',\n",
       " 'Chicago Freestyle (feat. Giveon)',\n",
       " 'Furthest Thing',\n",
       " 'Losses',\n",
       " 'Doing It Wrong',\n",
       " 'Jungle']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_recommended_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4422',\n",
       " '9',\n",
       " 'A Keeper',\n",
       " 'Brand New',\n",
       " \"Bria's Interlude (feat. Omarion)\",\n",
       " 'Broke Boys',\n",
       " \"Cece's Interlude\",\n",
       " 'Change Locations',\n",
       " 'Come and See Me (feat. Drake)',\n",
       " 'Company',\n",
       " 'Congratulations',\n",
       " 'Crew Love',\n",
       " 'D4L',\n",
       " 'Deep Pockets',\n",
       " 'Demons (feat. Fivio Foreign & Sosa Geek)',\n",
       " 'Don’t Matter To Me (with Michael Jackson)',\n",
       " 'Draft Day',\n",
       " 'Dreams Money Can Buy',\n",
       " 'Ela É do Tipo (feat. Drake) [Remix]',\n",
       " 'Faithful',\n",
       " 'Final Fantasy',\n",
       " 'Fire & Desire',\n",
       " \"Flight's Booked\",\n",
       " 'From Florida With Love',\n",
       " 'From Time',\n",
       " 'GREECE (feat. Drake)',\n",
       " 'Get Along Better',\n",
       " 'Get It Together',\n",
       " 'Girls Love Beyoncé (feat. James Fauntleroy)',\n",
       " 'Girls Want Girls (with Lil Baby)',\n",
       " \"God's Plan\",\n",
       " 'Gyalchester',\n",
       " \"Hold On, We're Going Home\",\n",
       " 'Hours In Silence',\n",
       " 'Houstatlantavegas',\n",
       " 'How Bout Now',\n",
       " 'I Get Lonely',\n",
       " 'I Guess It’s Fuck Me',\n",
       " \"I'm The Plug\",\n",
       " 'IMY2 (with Kid Cudi)',\n",
       " 'In My Feelings',\n",
       " 'Jersey',\n",
       " 'Jodeci Freestyle (feat. J. Cole)',\n",
       " 'Jorja Interlude',\n",
       " 'Jumpman',\n",
       " 'Jungle',\n",
       " 'KMT',\n",
       " 'Keep The Family Close',\n",
       " 'Knife Talk (with 21 Savage ft. Project Pat)',\n",
       " 'LOYAL (feat. Drake)',\n",
       " 'Laugh Now Cry Later (feat. Lil Durk)',\n",
       " 'Life Is Good (feat. Drake)',\n",
       " 'Light Up',\n",
       " 'Live From The Gutter',\n",
       " 'Losses',\n",
       " 'Lust For Life',\n",
       " 'MIA (feat. Drake)',\n",
       " 'Madonna',\n",
       " 'Massive',\n",
       " 'Miss Me',\n",
       " 'Mob Ties',\n",
       " 'No Frauds',\n",
       " 'Nonstop',\n",
       " 'Not You Too (feat. Chris Brown)',\n",
       " 'Nothings Into Somethings',\n",
       " 'On BS',\n",
       " 'One Dance',\n",
       " 'Over',\n",
       " 'Overdrive',\n",
       " 'Own It',\n",
       " 'Paris Morton Music',\n",
       " 'Pipe Down',\n",
       " 'Pop Style',\n",
       " 'Portland',\n",
       " 'Pound Cake / Paris Morton Music 2',\n",
       " 'Race My Mind',\n",
       " 'Ratchet Happy Birthday',\n",
       " 'Scholarships',\n",
       " 'Show Me A Good Time',\n",
       " 'Shut It Down',\n",
       " 'Started From the Bottom',\n",
       " 'Sticky',\n",
       " 'Still Here',\n",
       " 'Summer Games',\n",
       " 'TSU',\n",
       " 'Teenage Fever',\n",
       " 'That’s How You Feel',\n",
       " 'The Calm',\n",
       " 'The Language',\n",
       " 'The Resistance',\n",
       " 'Tie That Binds',\n",
       " 'Time Flies',\n",
       " 'Too Good',\n",
       " 'Toosie Slide',\n",
       " 'Trust Issues',\n",
       " 'Under Ground Kings',\n",
       " 'Wanna Know Remix (feat. Drake)',\n",
       " 'Wednesday Night Interlude',\n",
       " 'What’s Next',\n",
       " 'With You',\n",
       " 'You’re Mines Still (feat. Drake)'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(recommendations) & set(true_recommended_songs)) / len(true_recommended_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Calculate aggregate metrics\n",
    "mean_precision = sum(precision_list) / len(precision_list)\n",
    "mean_recall = sum(recall_list) / len(recall_list)\n",
    "mean_f1 = sum(f1_list) / len(f1_list)\n",
    "\n",
    "# Step 7: Interpret and analyze the results\n",
    "print(f\"Mean Precision: {mean_precision}\")\n",
    "print(f\"Mean Recall: {mean_recall}\")\n",
    "print(f\"Mean F1-score: {mean_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
